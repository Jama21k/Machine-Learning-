{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4b1cb77-5344-418c-94b3-7bacb800c746",
   "metadata": {},
   "source": [
    "# \n",
    "We are using a dataset on beer purchases. The goal is to predict if light beer purchased in the US is BUD light. To achieve this goal, we will use the information provided by the following socioeconomic characteristics:\n",
    "* market           - where the beer is bought\n",
    "* buyertype        - who is the buyer () \n",
    "* income           - ranges of income\n",
    "* childrenUnder6   - does the buyer have children under 6 years old\n",
    "* children6to17    - does the buyer have children between 6 and 17\n",
    "* age              - bracketed age groups\n",
    "* employment       - fully employed, partially employed, no employment.\n",
    "* degree           - level of occupation\n",
    "* occuptation      - which sector you are employed in\n",
    "* ethnic           - white, asian, hispanic, black or other\n",
    "* microwave        - own a microwave\n",
    "* dishwasher       - own a dishwasher\n",
    "* tvcable          - what type cable tv subscription you have\n",
    "* singlefamilyhome - are you in a single family home\n",
    "* npeople          - number of people you live with 1,2,3,4, +5\n",
    "\n",
    "First, we load the dataset and create an output variable that indicates purchases of Bud Light."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "620cb558-0655-4ff6-80d8-51cc89e3c96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    " \n",
    "file_path = r\"C:\\Users\\jamak\\OneDrive\\Lund\\Machine Learning\\LightBeer2.csv\"\n",
    "lb    = pd.read_csv(file_path)\n",
    "y     = np.zeros(shape=lb.shape[0])\n",
    "y[lb['beer_brand'] == \"BUD LIGHT\"]     = 1\n",
    "demog = lb.iloc[:,9:]\n",
    "demog = pd.get_dummies(demog, drop_first=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88174fd7-d779-4207-a30e-224c89bdcbc9",
   "metadata": {},
   "source": [
    "We also split the data into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a5c287b-7069-426c-b3c1-15857ff123f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(demog, y, train_size=0.75, shuffle=False)\n",
    "\n",
    "stdz_X = StandardScaler().fit(X_train)\n",
    "\n",
    "X_train = stdz_X.transform(X_train)\n",
    "X_test  = stdz_X.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25983d48-57e5-4342-9935-16676fd3a106",
   "metadata": {},
   "source": [
    "## Part 1: Specifying and training neural networks\n",
    "We will now start building a neural network to predict the purchase of Bud Light."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224c7cab-178e-4fee-b942-8ab68a140e23",
   "metadata": {},
   "source": [
    "We start with specifying the architecture of our very first and very small neural net `model1`.\n",
    "Add three layers to `model1`, two hidden layers with $30$ and $15$ hidden units, respectively, and an output layer.\n",
    "For the two hidden layers you should use the ReLU activation function. Additionally, We will choose an approriate activation function for the classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7d9388d-cbd6-4d10-90ff-2c3d974a8238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "# Initialize first moodel\n",
    "model1 = keras.Sequential()\n",
    "\n",
    "# add input layer\n",
    "model1.add(Input(shape=(X_train.shape[1],)))\n",
    "\n",
    "# Add first hidden layer with 30 hidden units\n",
    "model1.add(Dense(30, activation='relu'))\n",
    "\n",
    "# Add second hidden layer with 15 hidden units\n",
    "model1.add(Dense(15, activation='relu'))\n",
    "\n",
    "# Add output layer Binary classification thus sigmoid as activation function\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bcd214-22bf-42fa-a973-2c27f0643431",
   "metadata": {},
   "source": [
    "\n",
    "Next, we compile our model specification. We will choose a suitable loss function for our classification problem. As optimization algorithm we wil use *Adam* with learning rate $0.00003$. Lastly, use `accuracy` as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a88ddd5-3e85-4238-96a3-81b914f4fc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "# Model with binarycrossentropy as loss Adam as optimzer and learning rate of 00003\n",
    "model1.compile(optimizer= Adam(learning_rate=0.00003),\n",
    "               loss=BinaryCrossentropy(),\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2904463-9a53-4ea3-b175-adfc9ff0f599",
   "metadata": {},
   "source": [
    "# Training of Model \n",
    "Now we wil train the model using $250$ epochs, a batch_size of $2^8$, and use $25\\%$ of the data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f00f52b-92c7-4100-bfe0-05cdd6f0fe63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.4460 - loss: 0.8321 - val_accuracy: 0.4217 - val_loss: 0.8089\n",
      "Epoch 2/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4848 - loss: 0.7767 - val_accuracy: 0.5241 - val_loss: 0.7122\n",
      "Epoch 3/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5185 - loss: 0.7458 - val_accuracy: 0.6008 - val_loss: 0.6463\n",
      "Epoch 4/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5344 - loss: 0.7194 - val_accuracy: 0.6701 - val_loss: 0.6049\n",
      "Epoch 5/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5485 - loss: 0.7046 - val_accuracy: 0.7180 - val_loss: 0.5781\n",
      "Epoch 6/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5604 - loss: 0.6913 - val_accuracy: 0.7650 - val_loss: 0.5597\n",
      "Epoch 7/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5667 - loss: 0.6807 - val_accuracy: 0.7932 - val_loss: 0.5498\n",
      "Epoch 8/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5758 - loss: 0.6759 - val_accuracy: 0.7982 - val_loss: 0.5424\n",
      "Epoch 9/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5950 - loss: 0.6636 - val_accuracy: 0.8171 - val_loss: 0.5382\n",
      "Epoch 10/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6156 - loss: 0.6586 - val_accuracy: 0.8358 - val_loss: 0.5334\n",
      "Epoch 11/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6236 - loss: 0.6507 - val_accuracy: 0.8392 - val_loss: 0.5328\n",
      "Epoch 12/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6238 - loss: 0.6471 - val_accuracy: 0.8500 - val_loss: 0.5314\n",
      "Epoch 13/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6260 - loss: 0.6439 - val_accuracy: 0.8548 - val_loss: 0.5298\n",
      "Epoch 14/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6364 - loss: 0.6387 - val_accuracy: 0.8568 - val_loss: 0.5289\n",
      "Epoch 15/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6428 - loss: 0.6317 - val_accuracy: 0.8574 - val_loss: 0.5295\n",
      "Epoch 16/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6427 - loss: 0.6302 - val_accuracy: 0.8596 - val_loss: 0.5301\n",
      "Epoch 17/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6541 - loss: 0.6244 - val_accuracy: 0.8500 - val_loss: 0.5300\n",
      "Epoch 18/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6594 - loss: 0.6218 - val_accuracy: 0.8469 - val_loss: 0.5309\n",
      "Epoch 19/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6585 - loss: 0.6205 - val_accuracy: 0.8455 - val_loss: 0.5306\n",
      "Epoch 20/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6650 - loss: 0.6148 - val_accuracy: 0.8403 - val_loss: 0.5301\n",
      "Epoch 21/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6711 - loss: 0.6117 - val_accuracy: 0.8385 - val_loss: 0.5303\n",
      "Epoch 22/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6760 - loss: 0.6069 - val_accuracy: 0.8315 - val_loss: 0.5336\n",
      "Epoch 23/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6788 - loss: 0.6065 - val_accuracy: 0.8351 - val_loss: 0.5327\n",
      "Epoch 24/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6764 - loss: 0.6062 - val_accuracy: 0.8269 - val_loss: 0.5346\n",
      "Epoch 25/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6824 - loss: 0.6003 - val_accuracy: 0.8222 - val_loss: 0.5354\n",
      "Epoch 26/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6791 - loss: 0.6015 - val_accuracy: 0.8163 - val_loss: 0.5356\n",
      "Epoch 27/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6843 - loss: 0.5958 - val_accuracy: 0.8116 - val_loss: 0.5367\n",
      "Epoch 28/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6805 - loss: 0.5987 - val_accuracy: 0.8095 - val_loss: 0.5359\n",
      "Epoch 29/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6861 - loss: 0.5945 - val_accuracy: 0.8083 - val_loss: 0.5382\n",
      "Epoch 30/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6899 - loss: 0.5923 - val_accuracy: 0.8084 - val_loss: 0.5393\n",
      "Epoch 31/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6901 - loss: 0.5918 - val_accuracy: 0.8078 - val_loss: 0.5398\n",
      "Epoch 32/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6902 - loss: 0.5879 - val_accuracy: 0.8052 - val_loss: 0.5402\n",
      "Epoch 33/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6883 - loss: 0.5880 - val_accuracy: 0.8042 - val_loss: 0.5399\n",
      "Epoch 34/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6956 - loss: 0.5836 - val_accuracy: 0.8012 - val_loss: 0.5414\n",
      "Epoch 35/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6952 - loss: 0.5827 - val_accuracy: 0.7900 - val_loss: 0.5408\n",
      "Epoch 36/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6914 - loss: 0.5859 - val_accuracy: 0.7896 - val_loss: 0.5420\n",
      "Epoch 37/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7003 - loss: 0.5778 - val_accuracy: 0.7876 - val_loss: 0.5422\n",
      "Epoch 38/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6948 - loss: 0.5795 - val_accuracy: 0.7848 - val_loss: 0.5435\n",
      "Epoch 39/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6963 - loss: 0.5795 - val_accuracy: 0.7876 - val_loss: 0.5423\n",
      "Epoch 40/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7022 - loss: 0.5755 - val_accuracy: 0.7813 - val_loss: 0.5435\n",
      "Epoch 41/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6995 - loss: 0.5753 - val_accuracy: 0.7728 - val_loss: 0.5441\n",
      "Epoch 42/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7033 - loss: 0.5747 - val_accuracy: 0.7724 - val_loss: 0.5434\n",
      "Epoch 43/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7059 - loss: 0.5711 - val_accuracy: 0.7683 - val_loss: 0.5447\n",
      "Epoch 44/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7058 - loss: 0.5708 - val_accuracy: 0.7659 - val_loss: 0.5451\n",
      "Epoch 45/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7037 - loss: 0.5719 - val_accuracy: 0.7653 - val_loss: 0.5446\n",
      "Epoch 46/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7052 - loss: 0.5699 - val_accuracy: 0.7643 - val_loss: 0.5455\n",
      "Epoch 47/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7090 - loss: 0.5664 - val_accuracy: 0.7626 - val_loss: 0.5461\n",
      "Epoch 48/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7050 - loss: 0.5686 - val_accuracy: 0.7619 - val_loss: 0.5458\n",
      "Epoch 49/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7128 - loss: 0.5619 - val_accuracy: 0.7611 - val_loss: 0.5470\n",
      "Epoch 50/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7117 - loss: 0.5623 - val_accuracy: 0.7606 - val_loss: 0.5492\n",
      "Epoch 51/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7150 - loss: 0.5606 - val_accuracy: 0.7619 - val_loss: 0.5490\n",
      "Epoch 52/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7184 - loss: 0.5585 - val_accuracy: 0.7600 - val_loss: 0.5474\n",
      "Epoch 53/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7148 - loss: 0.5615 - val_accuracy: 0.7597 - val_loss: 0.5472\n",
      "Epoch 54/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7185 - loss: 0.5570 - val_accuracy: 0.7590 - val_loss: 0.5468\n",
      "Epoch 55/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7189 - loss: 0.5566 - val_accuracy: 0.7585 - val_loss: 0.5476\n",
      "Epoch 56/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7234 - loss: 0.5517 - val_accuracy: 0.7548 - val_loss: 0.5491\n",
      "Epoch 57/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7197 - loss: 0.5567 - val_accuracy: 0.7501 - val_loss: 0.5474\n",
      "Epoch 58/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7169 - loss: 0.5552 - val_accuracy: 0.7493 - val_loss: 0.5473\n",
      "Epoch 59/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7208 - loss: 0.5529 - val_accuracy: 0.7466 - val_loss: 0.5503\n",
      "Epoch 60/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7174 - loss: 0.5536 - val_accuracy: 0.7484 - val_loss: 0.5473\n",
      "Epoch 61/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7217 - loss: 0.5512 - val_accuracy: 0.7484 - val_loss: 0.5485\n",
      "Epoch 62/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7240 - loss: 0.5483 - val_accuracy: 0.7472 - val_loss: 0.5510\n",
      "Epoch 63/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7236 - loss: 0.5484 - val_accuracy: 0.7483 - val_loss: 0.5481\n",
      "Epoch 64/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7257 - loss: 0.5460 - val_accuracy: 0.7476 - val_loss: 0.5495\n",
      "Epoch 65/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7305 - loss: 0.5438 - val_accuracy: 0.7476 - val_loss: 0.5495\n",
      "Epoch 66/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7262 - loss: 0.5463 - val_accuracy: 0.7503 - val_loss: 0.5491\n",
      "Epoch 67/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7317 - loss: 0.5418 - val_accuracy: 0.7497 - val_loss: 0.5506\n",
      "Epoch 68/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7338 - loss: 0.5404 - val_accuracy: 0.7521 - val_loss: 0.5480\n",
      "Epoch 69/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7325 - loss: 0.5413 - val_accuracy: 0.7479 - val_loss: 0.5496\n",
      "Epoch 70/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7331 - loss: 0.5415 - val_accuracy: 0.7472 - val_loss: 0.5504\n",
      "Epoch 71/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7345 - loss: 0.5406 - val_accuracy: 0.7499 - val_loss: 0.5475\n",
      "Epoch 72/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7377 - loss: 0.5365 - val_accuracy: 0.7456 - val_loss: 0.5517\n",
      "Epoch 73/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7324 - loss: 0.5409 - val_accuracy: 0.7467 - val_loss: 0.5491\n",
      "Epoch 74/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7389 - loss: 0.5350 - val_accuracy: 0.7485 - val_loss: 0.5521\n",
      "Epoch 75/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7418 - loss: 0.5335 - val_accuracy: 0.7482 - val_loss: 0.5510\n",
      "Epoch 76/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7414 - loss: 0.5304 - val_accuracy: 0.7482 - val_loss: 0.5508\n",
      "Epoch 77/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7378 - loss: 0.5353 - val_accuracy: 0.7493 - val_loss: 0.5520\n",
      "Epoch 78/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7445 - loss: 0.5294 - val_accuracy: 0.7499 - val_loss: 0.5509\n",
      "Epoch 79/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7401 - loss: 0.5309 - val_accuracy: 0.7497 - val_loss: 0.5510\n",
      "Epoch 80/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7458 - loss: 0.5250 - val_accuracy: 0.7500 - val_loss: 0.5500\n",
      "Epoch 81/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7420 - loss: 0.5287 - val_accuracy: 0.7500 - val_loss: 0.5501\n",
      "Epoch 82/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7414 - loss: 0.5274 - val_accuracy: 0.7500 - val_loss: 0.5502\n",
      "Epoch 83/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7451 - loss: 0.5266 - val_accuracy: 0.7500 - val_loss: 0.5498\n",
      "Epoch 84/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7415 - loss: 0.5281 - val_accuracy: 0.7479 - val_loss: 0.5528\n",
      "Epoch 85/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7438 - loss: 0.5250 - val_accuracy: 0.7475 - val_loss: 0.5530\n",
      "Epoch 86/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7480 - loss: 0.5210 - val_accuracy: 0.7464 - val_loss: 0.5556\n",
      "Epoch 87/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7484 - loss: 0.5191 - val_accuracy: 0.7452 - val_loss: 0.5531\n",
      "Epoch 88/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7491 - loss: 0.5181 - val_accuracy: 0.7447 - val_loss: 0.5539\n",
      "Epoch 89/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7446 - loss: 0.5236 - val_accuracy: 0.7472 - val_loss: 0.5503\n",
      "Epoch 90/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7510 - loss: 0.5193 - val_accuracy: 0.7453 - val_loss: 0.5514\n",
      "Epoch 91/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7484 - loss: 0.5196 - val_accuracy: 0.7443 - val_loss: 0.5548\n",
      "Epoch 92/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7516 - loss: 0.5153 - val_accuracy: 0.7443 - val_loss: 0.5534\n",
      "Epoch 93/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7464 - loss: 0.5204 - val_accuracy: 0.7445 - val_loss: 0.5530\n",
      "Epoch 94/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7481 - loss: 0.5183 - val_accuracy: 0.7443 - val_loss: 0.5571\n",
      "Epoch 95/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7507 - loss: 0.5184 - val_accuracy: 0.7478 - val_loss: 0.5526\n",
      "Epoch 96/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7524 - loss: 0.5150 - val_accuracy: 0.7446 - val_loss: 0.5563\n",
      "Epoch 97/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7539 - loss: 0.5123 - val_accuracy: 0.7438 - val_loss: 0.5569\n",
      "Epoch 98/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7560 - loss: 0.5133 - val_accuracy: 0.7445 - val_loss: 0.5560\n",
      "Epoch 99/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7498 - loss: 0.5157 - val_accuracy: 0.7440 - val_loss: 0.5555\n",
      "Epoch 100/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7548 - loss: 0.5114 - val_accuracy: 0.7436 - val_loss: 0.5570\n",
      "Epoch 101/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7581 - loss: 0.5094 - val_accuracy: 0.7437 - val_loss: 0.5579\n",
      "Epoch 102/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7564 - loss: 0.5108 - val_accuracy: 0.7437 - val_loss: 0.5569\n",
      "Epoch 103/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7601 - loss: 0.5069 - val_accuracy: 0.7437 - val_loss: 0.5560\n",
      "Epoch 104/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7569 - loss: 0.5107 - val_accuracy: 0.7435 - val_loss: 0.5562\n",
      "Epoch 105/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7607 - loss: 0.5061 - val_accuracy: 0.7440 - val_loss: 0.5545\n",
      "Epoch 106/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7596 - loss: 0.5063 - val_accuracy: 0.7417 - val_loss: 0.5600\n",
      "Epoch 107/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7622 - loss: 0.5058 - val_accuracy: 0.7453 - val_loss: 0.5553\n",
      "Epoch 108/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7616 - loss: 0.5066 - val_accuracy: 0.7429 - val_loss: 0.5586\n",
      "Epoch 109/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7612 - loss: 0.5040 - val_accuracy: 0.7423 - val_loss: 0.5606\n",
      "Epoch 110/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7616 - loss: 0.5031 - val_accuracy: 0.7475 - val_loss: 0.5585\n",
      "Epoch 111/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7619 - loss: 0.5010 - val_accuracy: 0.7472 - val_loss: 0.5588\n",
      "Epoch 112/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7581 - loss: 0.5053 - val_accuracy: 0.7518 - val_loss: 0.5607\n",
      "Epoch 113/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7638 - loss: 0.5004 - val_accuracy: 0.7517 - val_loss: 0.5587\n",
      "Epoch 114/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7627 - loss: 0.5013 - val_accuracy: 0.7516 - val_loss: 0.5589\n",
      "Epoch 115/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7647 - loss: 0.4980 - val_accuracy: 0.7414 - val_loss: 0.5624\n",
      "Epoch 116/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7657 - loss: 0.4970 - val_accuracy: 0.7395 - val_loss: 0.5643\n",
      "Epoch 117/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7662 - loss: 0.4982 - val_accuracy: 0.7397 - val_loss: 0.5633\n",
      "Epoch 118/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7667 - loss: 0.4969 - val_accuracy: 0.7413 - val_loss: 0.5624\n",
      "Epoch 119/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7669 - loss: 0.4925 - val_accuracy: 0.7353 - val_loss: 0.5635\n",
      "Epoch 120/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7647 - loss: 0.4976 - val_accuracy: 0.7375 - val_loss: 0.5599\n",
      "Epoch 121/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7667 - loss: 0.4940 - val_accuracy: 0.7354 - val_loss: 0.5638\n",
      "Epoch 122/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7637 - loss: 0.4989 - val_accuracy: 0.7353 - val_loss: 0.5642\n",
      "Epoch 123/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7717 - loss: 0.4891 - val_accuracy: 0.7325 - val_loss: 0.5643\n",
      "Epoch 124/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7657 - loss: 0.4919 - val_accuracy: 0.7350 - val_loss: 0.5632\n",
      "Epoch 125/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7687 - loss: 0.4912 - val_accuracy: 0.7315 - val_loss: 0.5656\n",
      "Epoch 126/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7654 - loss: 0.4901 - val_accuracy: 0.7298 - val_loss: 0.5641\n",
      "Epoch 127/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7647 - loss: 0.4937 - val_accuracy: 0.7298 - val_loss: 0.5648\n",
      "Epoch 128/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7638 - loss: 0.4899 - val_accuracy: 0.7259 - val_loss: 0.5683\n",
      "Epoch 129/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7684 - loss: 0.4889 - val_accuracy: 0.7252 - val_loss: 0.5683\n",
      "Epoch 130/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7719 - loss: 0.4853 - val_accuracy: 0.7262 - val_loss: 0.5654\n",
      "Epoch 131/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7709 - loss: 0.4863 - val_accuracy: 0.7234 - val_loss: 0.5668\n",
      "Epoch 132/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7661 - loss: 0.4881 - val_accuracy: 0.7224 - val_loss: 0.5665\n",
      "Epoch 133/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7742 - loss: 0.4824 - val_accuracy: 0.7253 - val_loss: 0.5676\n",
      "Epoch 134/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7744 - loss: 0.4794 - val_accuracy: 0.7211 - val_loss: 0.5707\n",
      "Epoch 135/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7698 - loss: 0.4864 - val_accuracy: 0.7250 - val_loss: 0.5679\n",
      "Epoch 136/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7741 - loss: 0.4810 - val_accuracy: 0.7250 - val_loss: 0.5713\n",
      "Epoch 137/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7767 - loss: 0.4776 - val_accuracy: 0.7237 - val_loss: 0.5725\n",
      "Epoch 138/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7691 - loss: 0.4846 - val_accuracy: 0.7235 - val_loss: 0.5718\n",
      "Epoch 139/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7739 - loss: 0.4813 - val_accuracy: 0.7232 - val_loss: 0.5711\n",
      "Epoch 140/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7722 - loss: 0.4819 - val_accuracy: 0.7193 - val_loss: 0.5760\n",
      "Epoch 141/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7736 - loss: 0.4801 - val_accuracy: 0.7222 - val_loss: 0.5749\n",
      "Epoch 142/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7771 - loss: 0.4784 - val_accuracy: 0.7223 - val_loss: 0.5730\n",
      "Epoch 143/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7711 - loss: 0.4830 - val_accuracy: 0.7230 - val_loss: 0.5718\n",
      "Epoch 144/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7746 - loss: 0.4794 - val_accuracy: 0.7224 - val_loss: 0.5735\n",
      "Epoch 145/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7786 - loss: 0.4724 - val_accuracy: 0.7214 - val_loss: 0.5772\n",
      "Epoch 146/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7760 - loss: 0.4771 - val_accuracy: 0.7218 - val_loss: 0.5755\n",
      "Epoch 147/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7735 - loss: 0.4798 - val_accuracy: 0.7217 - val_loss: 0.5776\n",
      "Epoch 148/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7770 - loss: 0.4748 - val_accuracy: 0.7199 - val_loss: 0.5791\n",
      "Epoch 149/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7769 - loss: 0.4764 - val_accuracy: 0.7204 - val_loss: 0.5789\n",
      "Epoch 150/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7783 - loss: 0.4725 - val_accuracy: 0.7205 - val_loss: 0.5766\n",
      "Epoch 151/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7764 - loss: 0.4754 - val_accuracy: 0.7208 - val_loss: 0.5783\n",
      "Epoch 152/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7777 - loss: 0.4731 - val_accuracy: 0.7204 - val_loss: 0.5803\n",
      "Epoch 153/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7762 - loss: 0.4726 - val_accuracy: 0.7208 - val_loss: 0.5777\n",
      "Epoch 154/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7765 - loss: 0.4746 - val_accuracy: 0.7205 - val_loss: 0.5809\n",
      "Epoch 155/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7796 - loss: 0.4720 - val_accuracy: 0.7206 - val_loss: 0.5797\n",
      "Epoch 156/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7793 - loss: 0.4715 - val_accuracy: 0.7182 - val_loss: 0.5824\n",
      "Epoch 157/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7808 - loss: 0.4672 - val_accuracy: 0.7173 - val_loss: 0.5842\n",
      "Epoch 158/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7771 - loss: 0.4740 - val_accuracy: 0.7208 - val_loss: 0.5781\n",
      "Epoch 159/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7791 - loss: 0.4709 - val_accuracy: 0.7194 - val_loss: 0.5828\n",
      "Epoch 160/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7798 - loss: 0.4683 - val_accuracy: 0.7199 - val_loss: 0.5803\n",
      "Epoch 161/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7850 - loss: 0.4662 - val_accuracy: 0.7173 - val_loss: 0.5848\n",
      "Epoch 162/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7834 - loss: 0.4660 - val_accuracy: 0.7068 - val_loss: 0.5864\n",
      "Epoch 163/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7815 - loss: 0.4685 - val_accuracy: 0.7068 - val_loss: 0.5879\n",
      "Epoch 164/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7800 - loss: 0.4670 - val_accuracy: 0.7071 - val_loss: 0.5838\n",
      "Epoch 165/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7833 - loss: 0.4665 - val_accuracy: 0.7070 - val_loss: 0.5850\n",
      "Epoch 166/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7823 - loss: 0.4657 - val_accuracy: 0.7064 - val_loss: 0.5867\n",
      "Epoch 167/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7813 - loss: 0.4663 - val_accuracy: 0.7056 - val_loss: 0.5869\n",
      "Epoch 168/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7870 - loss: 0.4588 - val_accuracy: 0.7054 - val_loss: 0.5875\n",
      "Epoch 169/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7785 - loss: 0.4669 - val_accuracy: 0.7051 - val_loss: 0.5868\n",
      "Epoch 170/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7861 - loss: 0.4588 - val_accuracy: 0.7044 - val_loss: 0.5941\n",
      "Epoch 171/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7850 - loss: 0.4614 - val_accuracy: 0.7045 - val_loss: 0.5906\n",
      "Epoch 172/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7829 - loss: 0.4611 - val_accuracy: 0.7048 - val_loss: 0.5895\n",
      "Epoch 173/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7850 - loss: 0.4601 - val_accuracy: 0.7048 - val_loss: 0.5893\n",
      "Epoch 174/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7840 - loss: 0.4649 - val_accuracy: 0.7044 - val_loss: 0.5932\n",
      "Epoch 175/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7833 - loss: 0.4594 - val_accuracy: 0.7057 - val_loss: 0.5882\n",
      "Epoch 176/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7826 - loss: 0.4619 - val_accuracy: 0.7054 - val_loss: 0.5912\n",
      "Epoch 177/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7878 - loss: 0.4564 - val_accuracy: 0.7047 - val_loss: 0.5942\n",
      "Epoch 178/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7870 - loss: 0.4614 - val_accuracy: 0.7055 - val_loss: 0.5893\n",
      "Epoch 179/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7863 - loss: 0.4604 - val_accuracy: 0.7043 - val_loss: 0.5948\n",
      "Epoch 180/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7887 - loss: 0.4581 - val_accuracy: 0.7051 - val_loss: 0.5930\n",
      "Epoch 181/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7874 - loss: 0.4572 - val_accuracy: 0.7038 - val_loss: 0.5967\n",
      "Epoch 182/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7860 - loss: 0.4583 - val_accuracy: 0.7049 - val_loss: 0.5976\n",
      "Epoch 183/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7863 - loss: 0.4602 - val_accuracy: 0.7046 - val_loss: 0.5997\n",
      "Epoch 184/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7859 - loss: 0.4597 - val_accuracy: 0.7060 - val_loss: 0.5951\n",
      "Epoch 185/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7910 - loss: 0.4552 - val_accuracy: 0.7030 - val_loss: 0.6002\n",
      "Epoch 186/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7859 - loss: 0.4570 - val_accuracy: 0.7043 - val_loss: 0.5993\n",
      "Epoch 187/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7852 - loss: 0.4577 - val_accuracy: 0.7052 - val_loss: 0.5987\n",
      "Epoch 188/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7917 - loss: 0.4533 - val_accuracy: 0.7038 - val_loss: 0.5993\n",
      "Epoch 189/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7900 - loss: 0.4537 - val_accuracy: 0.7041 - val_loss: 0.5983\n",
      "Epoch 190/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7915 - loss: 0.4520 - val_accuracy: 0.7036 - val_loss: 0.5992\n",
      "Epoch 191/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7908 - loss: 0.4532 - val_accuracy: 0.7025 - val_loss: 0.6015\n",
      "Epoch 192/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7899 - loss: 0.4546 - val_accuracy: 0.7050 - val_loss: 0.5956\n",
      "Epoch 193/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7881 - loss: 0.4543 - val_accuracy: 0.7049 - val_loss: 0.5971\n",
      "Epoch 194/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7920 - loss: 0.4500 - val_accuracy: 0.7010 - val_loss: 0.6020\n",
      "Epoch 195/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7898 - loss: 0.4499 - val_accuracy: 0.6997 - val_loss: 0.6056\n",
      "Epoch 196/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7920 - loss: 0.4522 - val_accuracy: 0.7009 - val_loss: 0.6048\n",
      "Epoch 197/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7932 - loss: 0.4499 - val_accuracy: 0.6988 - val_loss: 0.6071\n",
      "Epoch 198/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7912 - loss: 0.4494 - val_accuracy: 0.6989 - val_loss: 0.6051\n",
      "Epoch 199/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7941 - loss: 0.4475 - val_accuracy: 0.6990 - val_loss: 0.6036\n",
      "Epoch 200/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7926 - loss: 0.4502 - val_accuracy: 0.6963 - val_loss: 0.6050\n",
      "Epoch 201/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7972 - loss: 0.4473 - val_accuracy: 0.7018 - val_loss: 0.6055\n",
      "Epoch 202/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7954 - loss: 0.4496 - val_accuracy: 0.7057 - val_loss: 0.6007\n",
      "Epoch 203/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7954 - loss: 0.4477 - val_accuracy: 0.7036 - val_loss: 0.6023\n",
      "Epoch 204/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7944 - loss: 0.4477 - val_accuracy: 0.6937 - val_loss: 0.6097\n",
      "Epoch 205/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7935 - loss: 0.4507 - val_accuracy: 0.7031 - val_loss: 0.6054\n",
      "Epoch 206/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7958 - loss: 0.4462 - val_accuracy: 0.6987 - val_loss: 0.6107\n",
      "Epoch 207/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7973 - loss: 0.4452 - val_accuracy: 0.6991 - val_loss: 0.6080\n",
      "Epoch 208/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7961 - loss: 0.4447 - val_accuracy: 0.7019 - val_loss: 0.6077\n",
      "Epoch 209/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7959 - loss: 0.4445 - val_accuracy: 0.7005 - val_loss: 0.6077\n",
      "Epoch 210/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7971 - loss: 0.4424 - val_accuracy: 0.7038 - val_loss: 0.6059\n",
      "Epoch 211/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7940 - loss: 0.4471 - val_accuracy: 0.7019 - val_loss: 0.6097\n",
      "Epoch 212/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7948 - loss: 0.4474 - val_accuracy: 0.6988 - val_loss: 0.6132\n",
      "Epoch 213/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7957 - loss: 0.4447 - val_accuracy: 0.7019 - val_loss: 0.6114\n",
      "Epoch 214/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7966 - loss: 0.4431 - val_accuracy: 0.7011 - val_loss: 0.6133\n",
      "Epoch 215/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7947 - loss: 0.4474 - val_accuracy: 0.7006 - val_loss: 0.6110\n",
      "Epoch 216/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7972 - loss: 0.4437 - val_accuracy: 0.7014 - val_loss: 0.6094\n",
      "Epoch 217/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7939 - loss: 0.4502 - val_accuracy: 0.6998 - val_loss: 0.6125\n",
      "Epoch 218/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7975 - loss: 0.4458 - val_accuracy: 0.7003 - val_loss: 0.6103\n",
      "Epoch 219/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7958 - loss: 0.4438 - val_accuracy: 0.6985 - val_loss: 0.6164\n",
      "Epoch 220/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7961 - loss: 0.4436 - val_accuracy: 0.6978 - val_loss: 0.6110\n",
      "Epoch 221/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8007 - loss: 0.4382 - val_accuracy: 0.7018 - val_loss: 0.6086\n",
      "Epoch 222/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8000 - loss: 0.4386 - val_accuracy: 0.6974 - val_loss: 0.6153\n",
      "Epoch 223/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7991 - loss: 0.4415 - val_accuracy: 0.6990 - val_loss: 0.6118\n",
      "Epoch 224/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7974 - loss: 0.4415 - val_accuracy: 0.6989 - val_loss: 0.6130\n",
      "Epoch 225/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7984 - loss: 0.4390 - val_accuracy: 0.7007 - val_loss: 0.6111\n",
      "Epoch 226/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8028 - loss: 0.4365 - val_accuracy: 0.6967 - val_loss: 0.6160\n",
      "Epoch 227/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8020 - loss: 0.4400 - val_accuracy: 0.6925 - val_loss: 0.6198\n",
      "Epoch 228/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8036 - loss: 0.4332 - val_accuracy: 0.6912 - val_loss: 0.6204\n",
      "Epoch 229/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8008 - loss: 0.4348 - val_accuracy: 0.6939 - val_loss: 0.6153\n",
      "Epoch 230/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8041 - loss: 0.4338 - val_accuracy: 0.6988 - val_loss: 0.6171\n",
      "Epoch 231/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7998 - loss: 0.4383 - val_accuracy: 0.6974 - val_loss: 0.6168\n",
      "Epoch 232/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8001 - loss: 0.4390 - val_accuracy: 0.7022 - val_loss: 0.6174\n",
      "Epoch 233/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8041 - loss: 0.4319 - val_accuracy: 0.6884 - val_loss: 0.6223\n",
      "Epoch 234/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7973 - loss: 0.4409 - val_accuracy: 0.6994 - val_loss: 0.6176\n",
      "Epoch 235/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8025 - loss: 0.4354 - val_accuracy: 0.6982 - val_loss: 0.6185\n",
      "Epoch 236/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8033 - loss: 0.4319 - val_accuracy: 0.6985 - val_loss: 0.6185\n",
      "Epoch 237/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8016 - loss: 0.4336 - val_accuracy: 0.7027 - val_loss: 0.6170\n",
      "Epoch 238/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8016 - loss: 0.4332 - val_accuracy: 0.6990 - val_loss: 0.6202\n",
      "Epoch 239/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8020 - loss: 0.4327 - val_accuracy: 0.6999 - val_loss: 0.6197\n",
      "Epoch 240/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8065 - loss: 0.4311 - val_accuracy: 0.6999 - val_loss: 0.6190\n",
      "Epoch 241/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8038 - loss: 0.4293 - val_accuracy: 0.6965 - val_loss: 0.6251\n",
      "Epoch 242/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8028 - loss: 0.4348 - val_accuracy: 0.6995 - val_loss: 0.6208\n",
      "Epoch 243/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8037 - loss: 0.4330 - val_accuracy: 0.6965 - val_loss: 0.6234\n",
      "Epoch 244/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8015 - loss: 0.4348 - val_accuracy: 0.6947 - val_loss: 0.6248\n",
      "Epoch 245/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8023 - loss: 0.4326 - val_accuracy: 0.6974 - val_loss: 0.6239\n",
      "Epoch 246/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8040 - loss: 0.4331 - val_accuracy: 0.6976 - val_loss: 0.6232\n",
      "Epoch 247/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8064 - loss: 0.4259 - val_accuracy: 0.6770 - val_loss: 0.6265\n",
      "Epoch 248/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8047 - loss: 0.4312 - val_accuracy: 0.6772 - val_loss: 0.6245\n",
      "Epoch 249/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8007 - loss: 0.4331 - val_accuracy: 0.6770 - val_loss: 0.6270\n",
      "Epoch 250/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8058 - loss: 0.4286 - val_accuracy: 0.6919 - val_loss: 0.6234\n"
     ]
    }
   ],
   "source": [
    "# Training model\n",
    "train = model1.fit(X_train, y_train,\n",
    "                     epochs=250,\n",
    "                     batch_size=2**8,\n",
    "                     validation_split= 0.25)\n",
    "\n",
    "loss_valloss_difference_1c = (\"We first see that the validation loss is decreasing, which is a clear indication that the model is effectivly learning\"\n",
    "                              \"As the learning continues zig-zag patterns arise (increases/decreases in val loss) even with adam's method patterns like this \"\n",
    "                              \"Here we probably run into saddle points, plateaus or local minima's. Adam is specifically designed to migate but can not entirely eliminate\"\n",
    "                              \"After around epoch 150 we see a gradually increase in the val error which indicates our model starts to overfit. We could implement\" \n",
    "                              \"methods like stopping rule to migtate this problem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74189128-f787-4b8c-8c96-6b782065dad7",
   "metadata": {},
   "source": [
    "### \n",
    "Early stopping can be used to avoid overfitting. We will apply this here, with `patience` argument set to 20.  `Model1b`, which otherwise should have a setup identical to `model1` as a copy of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f13f2cb-b29b-4288-9b8d-e69f331faff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5743 - loss: 0.6815 - val_accuracy: 0.7982 - val_loss: 0.5413\n",
      "Epoch 2/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6878 - loss: 0.5845 - val_accuracy: 0.7549 - val_loss: 0.5483\n",
      "Epoch 3/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7300 - loss: 0.5435 - val_accuracy: 0.7246 - val_loss: 0.5744\n",
      "Epoch 4/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7590 - loss: 0.5079 - val_accuracy: 0.7008 - val_loss: 0.5844\n",
      "Epoch 5/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7722 - loss: 0.4842 - val_accuracy: 0.6833 - val_loss: 0.6231\n",
      "Epoch 6/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7922 - loss: 0.4589 - val_accuracy: 0.6528 - val_loss: 0.6479\n",
      "Epoch 7/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8054 - loss: 0.4391 - val_accuracy: 0.6514 - val_loss: 0.6694\n",
      "Epoch 8/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8156 - loss: 0.4244 - val_accuracy: 0.6267 - val_loss: 0.7192\n",
      "Epoch 9/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8234 - loss: 0.4075 - val_accuracy: 0.6217 - val_loss: 0.7116\n",
      "Epoch 10/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8287 - loss: 0.3968 - val_accuracy: 0.6089 - val_loss: 0.7341\n",
      "Epoch 11/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8301 - loss: 0.3882 - val_accuracy: 0.6199 - val_loss: 0.7338\n",
      "Epoch 12/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8390 - loss: 0.3787 - val_accuracy: 0.6147 - val_loss: 0.7638\n",
      "Epoch 13/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8433 - loss: 0.3683 - val_accuracy: 0.5897 - val_loss: 0.8219\n",
      "Epoch 14/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8469 - loss: 0.3617 - val_accuracy: 0.6030 - val_loss: 0.7974\n",
      "Epoch 15/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8511 - loss: 0.3574 - val_accuracy: 0.5582 - val_loss: 0.9365\n",
      "Epoch 16/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8497 - loss: 0.3533 - val_accuracy: 0.6368 - val_loss: 0.7537\n",
      "Epoch 17/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8556 - loss: 0.3458 - val_accuracy: 0.6266 - val_loss: 0.8025\n",
      "Epoch 18/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8563 - loss: 0.3410 - val_accuracy: 0.6122 - val_loss: 0.8464\n",
      "Epoch 19/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8571 - loss: 0.3421 - val_accuracy: 0.5871 - val_loss: 0.9343\n",
      "Epoch 20/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8587 - loss: 0.3372 - val_accuracy: 0.6027 - val_loss: 0.8988\n",
      "Epoch 21/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8637 - loss: 0.3298 - val_accuracy: 0.6154 - val_loss: 0.8889\n",
      "Epoch 21: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define architecture here\n",
    "model1b = keras.Sequential()\n",
    "\n",
    "model1b.add(Input(shape=(X_train.shape[1],)))\n",
    "\n",
    "model1b.add(Dense(30, activation='relu'))\n",
    "\n",
    "model1b.add(Dense(15, activation='relu'))\n",
    "\n",
    "\n",
    "model1b.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model1b.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#  early stopping monitor\n",
    "early_stopping_monitor = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "model1b_fit = model1b_fit = model1b.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.25,\n",
    "    epochs=250,\n",
    "    batch_size=256,\n",
    "    callbacks=[early_stopping_monitor]\n",
    ")\n",
    "\n",
    "stopped_epoch = len(model1b_fit.history['loss'])\n",
    "\n",
    "when_earlystop_1d = f\"The training stopped after {stopped_epoch} epochs.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb727607-775d-44a3-a437-de1d7db42459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m572/572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step - accuracy: 0.5320 - loss: 0.7037\n",
      "[0.5843103528022766, 0.7067673206329346]\n"
     ]
    }
   ],
   "source": [
    "res_model1 = model1b.evaluate(X_test, y_test)\n",
    "print(res_model1)\n",
    "difference_in_accuracy_1e = \"The test accuracy is approx. 0.71 and validation accuracy is 0.62 so the difference is 0.09\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef98a6a-2952-4733-a42e-45a44cdcc7f9",
   "metadata": {},
   "source": [
    "# Model Evaluation \n",
    "Now we will use the `confusion_matrix()` function from the `metrics` module of scikit-learn to disaggregate model performance to class-specific performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d16957-1fcb-48b0-891c-9b778149090c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m572/572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 880us/step\n",
      "[[11064  1817]\n",
      " [ 3543  1855]]\n",
      "0.3389925373134328\n",
      "0.14358696493536652\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "prob_model1 = model1b.predict(X_test)\n",
    "CM_model1   = confusion_matrix(y_test, prob_model1 > 0.5)\n",
    "\n",
    "TN = CM_model1[0, 0]\n",
    "TP = CM_model1[0, 1]\n",
    "FN = CM_model1[1, 0]\n",
    "FP = CM_model1[1, 1]\n",
    "\n",
    "\n",
    "TPR_1f = TP / (TP + FN)\n",
    "FPR_1f = FP / (FP + TN)\n",
    "\n",
    "print(CM_model1)\n",
    "print(TPR_1f)\n",
    "print(FPR_1f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440be667",
   "metadata": {},
   "source": [
    "\n",
    "\"The model's performance varies significantly between classes. The True Positive Rate (TPR) of 38.2%\"\n",
    "                            \"suggests moderate effectiveness in identifying positive cases, as it correctly identifies about 38.2% of actual positives.\"\n",
    "                            \"However, the False Positive Rate (FPR) of 16.2% indicates that the model incorrectly labels 16.2% of negatives as positives,\"\n",
    "                            \"which might be problematic in scenarios where false positives are costly. These results suggest that the model's accuracy is\"\n",
    "                            \"not approximately equal across categories, with a specific weakness in distinguishing negatives without falsely labeling them as positives.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229b6fa2-89ad-4d87-bc6e-5031b13b1f7b",
   "metadata": {},
   "source": [
    "Here we will use $\\ell_2$ regularization to update the weights. `Model2` which is identical to that of `model1b` except for $\\ell_2$ regularization with regularization factor `l2_pen` in the two hidden layers.  \n",
    "\n",
    "Then we compile and fit this regularized model with the same parameters as previous method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7505b575-e5e1-4d4d-b42b-70960a1222d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6042 - loss: 0.9196 - val_accuracy: 0.8831 - val_loss: 0.6522\n",
      "Epoch 2/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6815 - loss: 0.7019 - val_accuracy: 0.7876 - val_loss: 0.5945\n",
      "Epoch 3/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7154 - loss: 0.6289 - val_accuracy: 0.6831 - val_loss: 0.6510\n",
      "Epoch 4/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7467 - loss: 0.5845 - val_accuracy: 0.6828 - val_loss: 0.6466\n",
      "Epoch 5/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7677 - loss: 0.5527 - val_accuracy: 0.6480 - val_loss: 0.6528\n",
      "Epoch 6/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7873 - loss: 0.5273 - val_accuracy: 0.6351 - val_loss: 0.7184\n",
      "Epoch 7/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7967 - loss: 0.5098 - val_accuracy: 0.6055 - val_loss: 0.7304\n",
      "Epoch 8/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8037 - loss: 0.4956 - val_accuracy: 0.6680 - val_loss: 0.7279\n",
      "Epoch 9/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8114 - loss: 0.4869 - val_accuracy: 0.6351 - val_loss: 0.7554\n",
      "Epoch 10/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8148 - loss: 0.4782 - val_accuracy: 0.6085 - val_loss: 0.7713\n",
      "Epoch 11/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8206 - loss: 0.4725 - val_accuracy: 0.6785 - val_loss: 0.7191\n",
      "Epoch 12/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8212 - loss: 0.4681 - val_accuracy: 0.6259 - val_loss: 0.7412\n",
      "Epoch 13/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8261 - loss: 0.4631 - val_accuracy: 0.6407 - val_loss: 0.7625\n",
      "Epoch 14/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8305 - loss: 0.4579 - val_accuracy: 0.6628 - val_loss: 0.7299\n",
      "Epoch 15/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8296 - loss: 0.4525 - val_accuracy: 0.6272 - val_loss: 0.7617\n",
      "Epoch 16/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8341 - loss: 0.4451 - val_accuracy: 0.5914 - val_loss: 0.8584\n",
      "Epoch 17/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8377 - loss: 0.4430 - val_accuracy: 0.6491 - val_loss: 0.7828\n",
      "Epoch 18/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8369 - loss: 0.4414 - val_accuracy: 0.6271 - val_loss: 0.7648\n",
      "Epoch 19/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8406 - loss: 0.4349 - val_accuracy: 0.6079 - val_loss: 0.8355\n",
      "Epoch 20/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8374 - loss: 0.4359 - val_accuracy: 0.6366 - val_loss: 0.7521\n",
      "Epoch 21/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8418 - loss: 0.4309 - val_accuracy: 0.6513 - val_loss: 0.7443\n",
      "Epoch 22/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8447 - loss: 0.4268 - val_accuracy: 0.6708 - val_loss: 0.7446\n",
      "Epoch 22: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "l2_pen = 0.005\n",
    "\n",
    "# 1.\n",
    "\n",
    "model2 = keras.Sequential()\n",
    "\n",
    "model2.add(Input(shape=(X_train.shape[1],)))\n",
    "\n",
    "model2.add(Dense(30, activation='relu', kernel_regularizer=l2(l2_pen)))\n",
    "\n",
    "model2.add(Dense(15, activation='relu', kernel_regularizer=l2(l2_pen)))\n",
    "\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# early stopping callback\n",
    "early_stopping_monitor = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 2\n",
    "model2_fit = model2.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.25,\n",
    "    epochs=250,\n",
    "    batch_size=256,\n",
    "    callbacks=[early_stopping_monitor]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74a8d24-9b08-48fe-b120-ca68106406fb",
   "metadata": {},
   "source": [
    "\n",
    "We compared the prediction accuracy on test and training sets. However, this is bad measure when the data is not well balanced (in terms of the observed output categories). Instead, one can use the cross entropy for the binomial distribution (minus the average log likelihood of the model). In fact, we chose this function as loss function for model training when we compiled `model1` and `model2`.\n",
    "\n",
    "To compare the test error of `model2` to that of `model1b` we don't want to use `loss` from `evaluate` since this includes the $\\ell_2$ penalty.  In the library `MLmetrics` the function `log_loss()` computes the cross entropy for the binomial distribution without penalty term. \n",
    "\n",
    "We will get predicted output probabilities on test data from `model2` and save them as `prob_model2`.\n",
    "Then we use `log_loss()` from the metrics module in scikit-learn to compute the cross-entropy loss for `model2` on the test data and save it as `logloss_model2`. \n",
    "Then we describe how the accuracy on test data differs between `model1b` and `model2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023a39ea-f599-4f1d-9ca0-4e54aa598cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m572/572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 806us/step\n",
      "0.5614786502640219\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# 1. \n",
    "prob_model2    = model2.predict(X_test)\n",
    "\n",
    "# 2.\n",
    "logloss_model2 = log_loss(y_test, prob_model2)\n",
    "print(logloss_model2)\n",
    "\n",
    "# 3.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cb2dac",
   "metadata": {},
   "source": [
    "\"The loss for the model including regularization(model2) is approx 0.54 which is better than the 0.59 of model 1b.\"\n",
    "                            \"This indicates model 2 is performing better in terms of genaralization\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059d8bf1-f86e-4b0c-9cc6-bdd9f1d964c4",
   "metadata": {},
   "source": [
    "## Part 2: Tuning neural nets with caret\n",
    "\n",
    "Keras provides functions that allow the use of scikit-learn for model tuning. Using this functionality requires relatively little effort and in this part we are going to practice the individual steps of model tuning with `sklearn`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ac151a-c3c3-44e8-8adb-a8a3cedd6d95",
   "metadata": {},
   "source": [
    "First, we need to define the architecture of the neural net that we tune and to compile the model. This needs to be done inside a function. The arguments of this function are the tuning parameters whose candidate values we want to feed into the function one by one.\n",
    "\n",
    "We will work with the architecture defined for `model2`. The only parameter that we want to tune is the regularization parameter for $\\ell_2$ penalization inside the hidden units.\n",
    "Now we create a function `modelbuild_2a` which has one argument `l2_pen`. In this function, specify the architecture of a Keras model `model`, identical to that of `model2` and with $\\ell_2$-penalty set to `l2_pen`. We Compile the model with the same settings and return it as function output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecfec657-d581-4b0c-b751-114dc1496386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the function to build the model with L2 regularization\n",
    "def modelbuild_2a(l2_pen):\n",
    "    model2 = Sequential([\n",
    "        Input(shape=(X_train.shape[1],)),\n",
    "        Dense(30, activation='relu', kernel_regularizer=l2(l2_pen)),\n",
    "        Dense(15, activation='relu', kernel_regularizer=l2(l2_pen)),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model2\n",
    "\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64078933-0ccc-4f9e-abb6-e26520bd8760",
   "metadata": {},
   "source": [
    "In order to make the function `modelbuild_2a` compatible with `sklearn`, we need to call it inside the `KerasClassifier()` function from the `wrappers` module of `scikit-learn`. I created a class due too problems with the module KerasCalssifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "027797af-2ee2-4566-8c67-ef98b0f40f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "\"\"\" Grid Search Cv kept cycling through all hyperparameter combinations in  a cycle disregarding the early stop,\n",
    " so I created a VerboseKeras Classifier to limit this.\n",
    "\n",
    "\"\"\"\n",
    "class VerboseKerasClassifier(KerasClassifier):\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        print(f\"Training model with l2_pen: {self.l2_pen}\")\n",
    "        return super().fit(X, y, **kwargs)\n",
    "\n",
    "model2_sklearn_spec = VerboseKerasClassifier(\n",
    "    model=modelbuild_2a,\n",
    "    l2_pen=0.005,\n",
    "    epochs=250,\n",
    "    batch_size=256,\n",
    "    validation_split=0.25,\n",
    "    verbose=0,\n",
    "    callbacks=[early_stopping_monitor]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3cce66-a557-4597-80dc-bb1ff84b4f1e",
   "metadata": {},
   "source": [
    "Next, we define a parameter grid `tune_grid_2c`. This must be a dictionary object.Its values is zero as well as $10^{r}$ for a grid of eleven $r$-values from $-4$ and $-1$ at equal distance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63925c0d-6abd-49e6-bfad-5adc320ce894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid for tuning l2_pen\n",
    "# Create L2 penalty values\n",
    "r_values = np.linspace(-4, -1, 11)\n",
    "l2_pen_values = np.concatenate(([0], 10**r_values))\n",
    "tune_grid_2c = {\n",
    "    'l2_pen': l2_pen_values[:3]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a978345-6081-478b-9549-771d300a1992",
   "metadata": {},
   "source": [
    "Now, we can tune our model. That's computationally quite costly, so we will use merely a fraction of the available training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e12d402-63b4-420b-a4b0-8c6f2830fe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_small,_ , y_train_small, _ = train_test_split(X_train, y_train, train_size=0.3, random_state=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69847933-232b-499f-847a-04255215d8cd",
   "metadata": {},
   "source": [
    "we will do the following:\n",
    "\n",
    "1. Use `Kfold()` from the model selection module in scikit-learn to define a random partition of the training data into five folds. and use $5$ as your random seed. \n",
    "2. Call `GridSearchCV()` and use the wrapper function from before, the parameter grid as well as `cv_splits_2d` as arguments.\n",
    "3. Apply the `fit()`-method to `GridSearchCV` and use `X_train_small` and `y_train_small` as inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8746fa6-0cd3-469c-afb2-7278ee02ae69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Training model with l2_pen: 0.0\n",
      "Epoch 51: early stopping\n",
      "Restoring model weights from the end of the best epoch: 31.\n",
      "[CV] END .........................................l2_pen=0.0; total time=   6.7s\n",
      "Training model with l2_pen: 0.0\n",
      "Epoch 67: early stopping\n",
      "Restoring model weights from the end of the best epoch: 47.\n",
      "[CV] END .........................................l2_pen=0.0; total time=   8.2s\n",
      "Training model with l2_pen: 0.0\n",
      "Epoch 57: early stopping\n",
      "Restoring model weights from the end of the best epoch: 37.\n",
      "[CV] END .........................................l2_pen=0.0; total time=   7.1s\n",
      "Training model with l2_pen: 0.0\n",
      "Epoch 52: early stopping\n",
      "Restoring model weights from the end of the best epoch: 32.\n",
      "[CV] END .........................................l2_pen=0.0; total time=   6.6s\n",
      "Training model with l2_pen: 0.0\n",
      "Epoch 58: early stopping\n",
      "Restoring model weights from the end of the best epoch: 38.\n",
      "[CV] END .........................................l2_pen=0.0; total time=   7.2s\n",
      "Training model with l2_pen: 0.0001\n",
      "Epoch 50: early stopping\n",
      "Restoring model weights from the end of the best epoch: 30.\n",
      "[CV] END ......................................l2_pen=0.0001; total time=   6.7s\n",
      "Training model with l2_pen: 0.0001\n",
      "Epoch 63: early stopping\n",
      "Restoring model weights from the end of the best epoch: 43.\n",
      "[CV] END ......................................l2_pen=0.0001; total time=   8.4s\n",
      "Training model with l2_pen: 0.0001\n",
      "Epoch 53: early stopping\n",
      "Restoring model weights from the end of the best epoch: 33.\n",
      "[CV] END ......................................l2_pen=0.0001; total time=   7.0s\n",
      "Training model with l2_pen: 0.0001\n",
      "Epoch 50: early stopping\n",
      "Restoring model weights from the end of the best epoch: 30.\n",
      "[CV] END ......................................l2_pen=0.0001; total time=   6.6s\n",
      "Training model with l2_pen: 0.0001\n",
      "Epoch 52: early stopping\n",
      "Restoring model weights from the end of the best epoch: 32.\n",
      "[CV] END ......................................l2_pen=0.0001; total time=   7.0s\n",
      "Training model with l2_pen: 0.00019952623149688788\n",
      "Epoch 54: early stopping\n",
      "Restoring model weights from the end of the best epoch: 34.\n",
      "[CV] END ......................l2_pen=0.00019952623149688788; total time=   6.8s\n",
      "Training model with l2_pen: 0.00019952623149688788\n",
      "Epoch 64: early stopping\n",
      "Restoring model weights from the end of the best epoch: 44.\n",
      "[CV] END ......................l2_pen=0.00019952623149688788; total time=   7.8s\n",
      "Training model with l2_pen: 0.00019952623149688788\n",
      "Epoch 48: early stopping\n",
      "Restoring model weights from the end of the best epoch: 28.\n",
      "[CV] END ......................l2_pen=0.00019952623149688788; total time=   6.3s\n",
      "Training model with l2_pen: 0.00019952623149688788\n",
      "Epoch 57: early stopping\n",
      "Restoring model weights from the end of the best epoch: 37.\n",
      "[CV] END ......................l2_pen=0.00019952623149688788; total time=   7.6s\n",
      "Training model with l2_pen: 0.00019952623149688788\n",
      "Epoch 49: early stopping\n",
      "Restoring model weights from the end of the best epoch: 29.\n",
      "[CV] END ......................l2_pen=0.00019952623149688788; total time=   6.8s\n",
      "Training model with l2_pen: 0.0\n",
      "Epoch 65: early stopping\n",
      "Restoring model weights from the end of the best epoch: 45.\n",
      "Cross validation finished in 117.5479838848114 seconds\n",
      "Best parameters: {'l2_pen': 0.0}\n",
      "Best score: 0.8062006079027355\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import (KFold, GridSearchCV)\n",
    "cv_splits_2d = KFold(n_splits=5, shuffle=True, random_state=5)\n",
    "import time\n",
    "\n",
    "#1 CV KFold \n",
    "cv_splits_2d = KFold(n_splits=5, shuffle=True, random_state=5)\n",
    "\n",
    "# 2 GridSearchCV setup\n",
    "NN_tune_2d = GridSearchCV(\n",
    "    estimator=model2_sklearn_spec,\n",
    "    param_grid=tune_grid_2c,\n",
    "    cv=cv_splits_2d,\n",
    "    verbose=2,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "# 3 Fit the model\n",
    "start_time = time.time()\n",
    "nn_trainlog = NN_tune_2d.fit(X_train_small, y_train_small)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Cross validation finished in {elapsed_time:} seconds\")\n",
    "print(\"Best parameters:\", NN_tune_2d.best_params_)\n",
    "print(\"Best score:\", NN_tune_2d.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c16a15-f9d0-4d38-896a-627e4d5dde75",
   "metadata": {},
   "source": [
    "By default, GridSearchCV saves the trained model with the best tuning parameter values as as object `best_estimator_` inside `NN_tune_2d`. However, in our case this model is a KerasClassifier object. We cannot use such an object for making predictions on test data. \n",
    "We will extract this object-inside-the-object-inside-the-object and save it as `model3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed6ec904-ce98-4907-b3fa-cc54b1a43cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = NN_tune_2d.best_estimator_.model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf6448b-ac63-4129-9236-6dcb23bcf1dd",
   "metadata": {},
   "source": [
    "## Part 3: Saving, loading and retraining neural nets \n",
    "\n",
    "Training and tuning neural nets can take a lot of time. Therefore, it is possible to save entire fitted models to disk and to import them at a later point in time. Apply the `save()`-method to your most recent `model3` in order to save it \n",
    "\n",
    "we will use the file explorer in your operating system to look how exactly the model was saved on my hard drive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "720e1779-a59c-4153-9c89-39d3be986fbc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m save_path = os.path.join(os.getcwd(), \u001b[33m'\u001b[39m\u001b[33mDABN13_asst6_saved_model3.keras\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mmodel3\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m(save_path)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Save the model to new name \u001b[39;00m\n\u001b[32m      8\u001b[39m saved_model_3a = save_path\n",
      "\u001b[31mAttributeError\u001b[39m: 'function' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "# Define the path \n",
    "save_path = os.path.join(os.getcwd(), 'DABN13_asst6_saved_model3.keras')\n",
    "\n",
    "# Save the model\n",
    "model3.save(save_path)\n",
    "\n",
    "# Save the model to new name \n",
    "saved_model_3a = save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3424edf9-37e1-4b9e-9c91-4836c6367709",
   "metadata": {},
   "source": [
    "Now, we use the `load_model` function in the `models` module of Keras to load your saved model into your python session again. Save this model as `model4`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f89a6c-9730-4dbd-950f-2d5b5f2f0da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "#Load saved model\n",
    "model4 = load_model('DABN13_asst6_saved_model3.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137e4808-0223-4b7f-9f9d-ef43482c2c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# set early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "# Retrain the model\n",
    "history = model4.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=250,\n",
    "    batch_size=2**8,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Get predicted probabilities\n",
    "prob_model4 = model4.predict(X_test)\n",
    "\n",
    "# Calculate log loss\n",
    "from sklearn.metrics import log_loss\n",
    "logloss_model4 = log_loss(y_test, prob_model4)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_model4 = model4.evaluate(X_test, y_test)[1]\n",
    "\n",
    "performance_comparison_3c = f\"\"\"\n",
    "The log loss of model4 on the test set is {logloss_model4:.6f}.\n",
    "The accuracy of model4 on the test set is {accuracy_model4:.6f}.\n",
    "\n",
    "Compared to model2 (which had an accuracy of {res_model1[1]:.6f}), \n",
    "model4s accuracy was higher at {accuracy_model4} comapred to {res_model1[1]:.6f}. \n",
    "The model improved by {abs(accuracy_model4 -res_model1[1]):.5f}. \n",
    "\n",
    "This performance improvement was due to the hyperparameter tuning and training\n",
    "on the full dataset. The hyperparameter tuning helped find a more optimal model architecture.\n",
    "\"\"\"\n",
    "print(performance_comparison_3c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7766e3e-925d-434b-a546-35b8a77fa177",
   "metadata": {},
   "source": [
    "## Part 4: Manual predictions from a trained neural net\n",
    "\n",
    "In this part we will build predictions *manually* by extracting weights from the trained  `model2` and by constructing the transformations in the layers of the neural net ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d3b6b3-6107-41d3-8bd4-09cc1c393791",
   "metadata": {},
   "source": [
    "creating our own ReLU activation function. Then, write our own sigmoid function for the output layer transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755df496-d94c-48b7-8a0d-d406e94d6e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation function\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72d8157-ef24-4994-a4bd-34827db28d8e",
   "metadata": {},
   "source": [
    "Now we are going to use the equations both hidden units and output unit to construct output predictions for the $n_{test}$ data points in `X_test`. WE WILL DO THE FOLLOWING\n",
    "\n",
    "1. Apply the `get_weights()` method on `model2` to obtain a list object which stores the weights and biases of the learned model. \n",
    "2. Extract the objects inside `weight_and_bias_4b` into the objects for $\\mathbf{b}_1,\\mathbf{W}_1, \\mathbf{b}_2, \\mathbf{W}_2,\\mathbf{b}_3, \\mathbf{W}_3$ defined in the code chunk below.\n",
    "3. Construct the linear term of the first layer hidden units and THEN save these linear terms as a $30 \\times n_{train}$ vector `Z_1`.\n",
    "4. Construct the $15 \\times n_{train}$ vector `Z_2` of linear terms for the second layer hidden units. Then, we obtain the linear term of the output unit and save it as `Z_3`.\n",
    "5. Put `Z_3` into the output layer activation function in order to get predictions for the probability of a Bud Light purchase. Save this as $n_{train} \\times 1$ vector `pred_own_2b`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50523588-05be-4174-b294-392f96cd8f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract weights and biases from the model 2\n",
    "weight_and_bias_4b = model2.get_weights()\n",
    "\n",
    "# 2. Extract weight and biases\n",
    "W1 = weight_and_bias_4b[0]  \n",
    "b1 = weight_and_bias_4b[1]  \n",
    "W2 = weight_and_bias_4b[2]  \n",
    "b2 = weight_and_bias_4b[3]  \n",
    "W3 = weight_and_bias_4b[4]  \n",
    "b3 = weight_and_bias_4b[5]  \n",
    "\n",
    "# 3. Linear term for the first hidden layer\n",
    "Z_1 = np.dot(X_test, W1) + b1\n",
    "A_1 = ReLU(Z_1) # Apply written Relu function\n",
    "\n",
    "# 4. Linear term for the second hidden layer\n",
    "Z_2 = np.dot(A_1, W2) + b2\n",
    "A_2 = ReLU(Z_2) \n",
    "#5 linear term for the output layer\n",
    "Z_3 = np.dot(A_2, W3) + b3\n",
    "\n",
    "# Apply sigmoid activation to Z_3\n",
    "prob_model2_own_4b = prob_model2_own_4b = sigmoid(Z_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
